{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import cPickle\n",
    "import h5py\n",
    "import argparse\n",
    "import soundfile\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(fd):\n",
    "    if not os.path.exists(fd):\n",
    "        os.makedirs(fd)\n",
    "\n",
    "def get_filename(path):\n",
    "    path = os.path.realpath(path)\n",
    "    na_ext = path.split('/')[-1]\n",
    "    na = os.path.splitext(na_ext)[0]\n",
    "    return na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(path, target_fs=None):\n",
    "    \"\"\"Read 1 dimension audio sequence from given path. \n",
    "    \n",
    "    Args:\n",
    "      path: string, path of audio. \n",
    "      target_fs: int, resampling rate. \n",
    "      \n",
    "    Returns:\n",
    "      audio: 1 dimension audio sequence. \n",
    "      fs: sampling rate of audio. \n",
    "    \"\"\"\n",
    "    (audio, fs) = soundfile.read(path)\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    if target_fs is not None and fs != target_fs:\n",
    "        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)\n",
    "        fs = target_fs\n",
    "    return audio, fs\n",
    "\n",
    "def write_audio(path, audio, sample_rate):\n",
    "    \"\"\"Write audio sequence to .wav file. \n",
    "    \n",
    "    Args:\n",
    "      path: string, path to write out .wav file. \n",
    "      data: ndarray, audio sequence to write out. \n",
    "      sample_rate: int, sample rate to write out. \n",
    "      \n",
    "    Returns: \n",
    "      None. \n",
    "    \"\"\"\n",
    "    soundfile.write(file=path, data=audio, samplerate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram(audio):\n",
    "    \"\"\"Calculate magnitude spectrogram of an audio sequence. \n",
    "    \n",
    "    Args: \n",
    "      audio: 1darray, audio sequence. \n",
    "      \n",
    "    Returns:\n",
    "      x: ndarray, spectrogram (n_time, n_freq)\n",
    "    \"\"\"\n",
    "    n_window = cfg.n_window\n",
    "    n_overlap = cfg.n_overlap\n",
    "    \n",
    "    ham_win = np.hamming(n_window)\n",
    "    [f, t, x] = signal.spectral.spectrogram(\n",
    "                    audio, \n",
    "                    window=ham_win,\n",
    "                    nperseg=n_window, \n",
    "                    noverlap=n_overlap, \n",
    "                    detrend=False, \n",
    "                    return_onesided=True, \n",
    "                    mode='magnitude') \n",
    "    x = x.T\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def logmel(audio):\n",
    "    \"\"\"Calculate log Mel spectrogram of an audio sequence. \n",
    "    \n",
    "    Args: \n",
    "      audio: 1darray, audio sequence. \n",
    "      \n",
    "    Returns:\n",
    "      x: ndarray, log Mel spectrogram (n_time, n_freq)\n",
    "    \"\"\"\n",
    "    n_window = cfg.n_window\n",
    "    n_overlap = cfg.n_overlap\n",
    "    fs = cfg.sample_rate\n",
    "    \n",
    "    ham_win = np.hamming(n_window)\n",
    "    [f, t, x] = signal.spectral.spectrogram(\n",
    "                    audio, \n",
    "                    window=ham_win,\n",
    "                    nperseg=n_window, \n",
    "                    noverlap=n_overlap, \n",
    "                    detrend=False, \n",
    "                    return_onesided=True, \n",
    "                    mode='magnitude') \n",
    "    x = x.T\n",
    "                    \n",
    "    if globals().get('melW') is None:\n",
    "        global melW\n",
    "        melW = librosa.filters.mel(sr=fs, \n",
    "                                n_fft=n_window, \n",
    "                                n_mels=229, \n",
    "                                fmin=0, \n",
    "                                fmax=fs / 2.)\n",
    "    x = np.dot(x, melW.T)\n",
    "    x = np.log(x + 1e-8)\n",
    "    x = x.astype(np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(args): \n",
    "    \"\"\"Calculate and write out features & ground truth notes of all songs in MUS \n",
    "    directory of all pianos. \n",
    "    \"\"\"\n",
    "    dataset_dir = args.dataset_dir\n",
    "    workspace = args.workspace\n",
    "    feat_type = args.feat_type\n",
    "    fs = cfg.sample_rate\n",
    "    tr_pianos = cfg.tr_pianos\n",
    "    te_pianos = cfg.te_pianos\n",
    "    pitch_bgn = cfg.pitch_bgn\n",
    "    pitch_fin = cfg.pitch_fin\n",
    "    \n",
    "    out_dir = os.path.join(workspace, \"features\", feat_type)\n",
    "    create_folder(out_dir)\n",
    "    \n",
    "    # Calculate features for all 9 pianos. \n",
    "    cnt = 0\n",
    "    for piano in tr_pianos + te_pianos:\n",
    "        audio_dir = os.path.join(dataset_dir, piano, \"MUS\")\n",
    "        wav_names = [na for na in os.listdir(audio_dir) if na.endswith('.wav')]\n",
    "        \n",
    "        for wav_na in wav_names:\n",
    "            # Read audio. \n",
    "            bare_na = os.path.splitext(wav_na)[0]\n",
    "            wav_path = os.path.join(audio_dir, wav_na)\n",
    "            (audio, _) = read_audio(wav_path, target_fs=fs)\n",
    "            \n",
    "            # Calculate feature. \n",
    "            if feat_type == \"spectrogram\":\n",
    "                x = spectrogram(audio)\n",
    "            elif feat_type == \"logmel\":\n",
    "                x = logmel(audio)\n",
    "            else:\n",
    "                raise Exception(\"Error!\")\n",
    "            \n",
    "            # Read piano roll from txt file. \n",
    "            (n_time, n_freq) = x.shape\n",
    "            txt_path = os.path.join(audio_dir, \"%s.txt\" % bare_na)\n",
    "            roll = txt_to_midi_roll(txt_path, max_fr_len=n_time)    # (n_time, 128)\n",
    "            y = roll[:, pitch_bgn : pitch_fin]      # (n_time, 88)\n",
    "            \n",
    "            # Write out data. \n",
    "            data = [x, y]\n",
    "            out_path = os.path.join(out_dir, \"%s.p\" % bare_na)\n",
    "            print(cnt, out_path, x.shape, y.shape)\n",
    "            cPickle.dump(data, open(out_path, 'wb'), protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in_pianos(na, list_of_piano):\n",
    "    \"\"\"E.g., na=\"MAPS_MUS-alb_esp2_SptkBGCl.wav\", list_of_piano=['SptkBGCl', ...]\n",
    "    then return True. \n",
    "    \"\"\"\n",
    "    for piano in list_of_piano:\n",
    "        if piano in na:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_features(args):\n",
    "    \"\"\"Pack already calculated features and write out to a big file, for \n",
    "    speeding up later loading. \n",
    "    \"\"\"\n",
    "    workspace = args.workspace\n",
    "    feat_type = args.feat_type\n",
    "    tr_pianos = cfg.tr_pianos\n",
    "    te_pianos = cfg.te_pianos\n",
    "    \n",
    "    fe_dir = os.path.join(workspace, \"features\", feat_type)\n",
    "    fe_names = os.listdir(fe_dir)\n",
    "    \n",
    "    # Load all single feature files and append to list. \n",
    "    tr_x_list, tr_y_list, tr_na_list = [], [], []\n",
    "    te_x_list, te_y_list, te_na_list = [], [], []\n",
    "    t1 = time.time()\n",
    "    cnt = 0\n",
    "    for fe_na in fe_names:\n",
    "        print(cnt)\n",
    "        bare_na = os.path.splitext(fe_na)[0]\n",
    "        fe_path = os.path.join(fe_dir, fe_na)\n",
    "        [x, y] = cPickle.load(open(fe_path, 'rb'))\n",
    "        \n",
    "        if is_in_pianos(fe_na, tr_pianos):\n",
    "            tr_x_list.append(x)\n",
    "            tr_y_list.append(y)\n",
    "            tr_na_list.append(\"%s.wav\" % bare_na)\n",
    "        elif is_in_pianos(fe_na, te_pianos):\n",
    "            te_x_list.append(x)\n",
    "            te_y_list.append(y)\n",
    "            te_na_list.append(\"%s.wav\" % bare_na)\n",
    "        else:\n",
    "            raise Exception(\"File not in tr_pianos or te_pianos!\")\n",
    "        cnt += 1\n",
    "    \n",
    "    # Write out the big file. \n",
    "    out_dir = os.path.join(workspace, \"packed_features\", feat_type)\n",
    "    create_folder(out_dir)\n",
    "    tr_packed_feat_path = os.path.join(out_dir, \"train.p\")\n",
    "    te_packed_feat_path = os.path.join(out_dir, \"test.p\")\n",
    "    \n",
    "    cPickle.dump([tr_x_list, tr_y_list, tr_na_list], open(tr_packed_feat_path, 'wb'), protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    cPickle.dump([te_x_list, te_y_list, te_na_list], open(te_packed_feat_path, 'wb'), protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Packing time: %s s\" % (time.time() - t1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scaler(args):\n",
    "    \"\"\"Compute and write out scaler from already packed feature file. Using \n",
    "    scaler in training neural network can speed up training. \n",
    "    \"\"\"\n",
    "    workspace = args.workspace\n",
    "    feat_type = args.feat_type\n",
    "    \n",
    "    # Load packed features. \n",
    "    t1 = time.time()\n",
    "    packed_feat_path = os.path.join(workspace, \"packed_features\", feat_type, \"train.p\")\n",
    "    [x_list, _, _] = cPickle.load(open(packed_feat_path, 'rb'))\n",
    "    \n",
    "    # Compute scaler. \n",
    "    x_all = np.concatenate(x_list)\n",
    "    scaler = preprocessing.StandardScaler(with_mean=True, with_std=True).fit(x_all)\n",
    "    print(scaler.mean_)\n",
    "    print(scaler.scale_)\n",
    "    \n",
    "    # Save out scaler. \n",
    "    out_path = os.path.join(workspace, \"scalers\", feat_type, \"scaler.p\")\n",
    "    create_folder(os.path.dirname(out_path))\n",
    "    pickle.dump(scaler, open(out_path, 'wb'))\n",
    "    print(\"Compute scaler finished! %s s\" % (time.time() - t1,))\n",
    "    \n",
    "def scale_on_x_list(x_list, scaler): \n",
    "    \"\"\"Scale list of ndarray. \n",
    "    \"\"\"\n",
    "    return [scaler.transform(e) for e in x_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_3d(x_list, y_list, n_concat, n_hop):\n",
    "    \"\"\"Convert data to 3d tensor. \n",
    "    \n",
    "    Args: \n",
    "      x_list: list of ndarray, e.g., [(N1, n_freq), (N2, n_freq), ...]\n",
    "      y_list: list of ndarray, e.g., [(N1, 88), (N2, 88), ...]\n",
    "      n_concat: int, number of frames to concatenate. \n",
    "      n_hop: int, hop frames. \n",
    "      \n",
    "    Returns:\n",
    "      x_all: (n_samples, n_concat, n_freq)\n",
    "      y_all: (n_samples, n_out)\n",
    "    \"\"\"\n",
    "    x_all, y_all = [], []\n",
    "    n_half = (n_concat - 1) / 2\n",
    "    for e in x_list:\n",
    "        x3d = mat_2d_to_3d(e, n_concat, n_hop)\n",
    "        x_all.append(x3d)\n",
    "        \n",
    "    for e in y_list:\n",
    "        y3d = mat_2d_to_3d(e, n_concat, n_hop)\n",
    "        y_all.append(y3d)\n",
    "        \n",
    "    x_all = np.concatenate(x_all, axis=0)   # (n_samples, n_concat, n_freq)\n",
    "    y_all = np.concatenate(y_all, axis=0)   # (n_samples, n_concat, n_out)\n",
    "    y_all = y_all[:, n_half, :]     # (n_samples, n_out)\n",
    "    return x_all, y_all\n",
    "    \n",
    "def mat_2d_to_3d(x, agg_num, hop):\n",
    "    \"\"\"Convert data to 3d tensor. \n",
    "    \n",
    "    Args: \n",
    "      x: 2darray, e.g., (N, n_in)\n",
    "      agg_num: int, number of frames to concatenate. \n",
    "      hop: int, hop frames. \n",
    "      \n",
    "    Returns:\n",
    "      x3d: 3darray, e.g., (n_samples, agg_num, n_in)\n",
    "    \"\"\"\n",
    "    # pad to at least one block\n",
    "    len_x, n_in = x.shape\n",
    "    if (len_x < agg_num):\n",
    "        x = np.concatenate((x, np.zeros((agg_num-len_x, n_in))))\n",
    "        \n",
    "    # agg 2d to 3d\n",
    "    len_x = len(x)\n",
    "    i1 = 0\n",
    "    x3d = []\n",
    "    while (i1+agg_num <= len_x):\n",
    "        x3d.append(x[i1:i1+agg_num])\n",
    "        i1 += hop\n",
    "    x3d = np.array(x3d)\n",
    "    return x3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_midi_roll(txt_path, max_fr_len):\n",
    "    \"\"\"Read txt to piano roll. \n",
    "    \n",
    "    Args: \n",
    "      txt_path: string, path of note info txt. \n",
    "      max_fr_len: int, should be the same as the number of frames of calculated \n",
    "          feature. \n",
    "          \n",
    "    Returns:\n",
    "      midi_roll: (n_time, 108)\n",
    "    \"\"\"\n",
    "    step_sec = cfg.step_sec\n",
    "    \n",
    "    with open(txt_path, 'rb') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        lis = list(reader)\n",
    "\n",
    "    midi_roll = np.zeros((max_fr_len, 128))\n",
    "    for i1 in xrange(1, len(lis)):\n",
    "        # Read a note info from a line. \n",
    "        [onset_time, offset_time, midi_pitch] = lis[i1]\n",
    "        onset_time = float(onset_time)\n",
    "        offset_time = float(offset_time)\n",
    "        midi_pitch = int(midi_pitch)\n",
    "        \n",
    "        # Write a note info to midi roll. \n",
    "        onset_fr = int(np.floor(onset_time / step_sec))\n",
    "        offset_fr = int(np.ceil(offset_time / step_sec)) + 1\n",
    "        midi_roll[onset_fr : offset_fr, midi_pitch] = 1\n",
    "        \n",
    "    return midi_roll\n",
    "\n",
    "def prob_to_midi_roll(x, thres):\n",
    "    \"\"\"Threshold input probability to binary, then convert piano roll (n_time, 88) \n",
    "    to midi roll (n_time, 108). \n",
    "    \n",
    "    Args:\n",
    "      x: (n_time, n_pitch)    \n",
    "    \"\"\"\n",
    "    pitch_bgn = cfg.pitch_bgn\n",
    "    x_bin = np.zeros_like(x)\n",
    "    x_bin[np.where(x >= thres)] = 1\n",
    "    n_time = x.shape[0]\n",
    "    out = np.zeros((n_time, 128))\n",
    "    out[:, pitch_bgn : pitch_bgn + 88] = x_bin\n",
    "    return out    \n",
    "\n",
    "def write_midi_roll_to_midi(x, out_path):\n",
    "    \"\"\"Write out midi_roll to midi file. \n",
    "    \n",
    "    Args: \n",
    "      x: (n_time, n_pitch), midi roll. \n",
    "      out_path: string, path to write out the midi. \n",
    "    \"\"\"\n",
    "    step_sec = cfg.step_sec\n",
    "    \n",
    "    def _get_bgn_fin_pairs(ary):\n",
    "        pairs = []\n",
    "        bgn_fr, fin_fr = -1, -1\n",
    "        for i2 in xrange(1, len(ary)):\n",
    "            if ary[i2-1] == 0 and ary[i2] == 0:\n",
    "                pass\n",
    "            elif ary[i2-1] == 0 and ary[i2] == 1:\n",
    "                bgn_fr = i2\n",
    "            elif ary[i2-1] == 1 and ary[i2] == 0:\n",
    "                fin_fr = i2\n",
    "                if fin_fr > bgn_fr:\n",
    "                    pairs.append((bgn_fr, fin_fr))\n",
    "            elif ary[i2-1] == 1 and ary[i2] == 1:\n",
    "                pass\n",
    "            else:\n",
    "                raise Exception(\"Input must be binary matrix!\")\n",
    "            \n",
    "        return pairs\n",
    "    \n",
    "    # Get (pitch, bgn_frame, fin_frame) triple. \n",
    "    triples = []\n",
    "    (_, n_pitch) = x.shape\n",
    "    for i1 in xrange(n_pitch):\n",
    "        ary = x[:, i1]\n",
    "        pairs_per_pitch = _get_bgn_fin_pairs(ary)\n",
    "        if pairs_per_pitch:\n",
    "            triples_per_pitch = [(i1,) + pair for pair in pairs_per_pitch]\n",
    "            triples += triples_per_pitch\n",
    "    \n",
    "    # Sort by begin frame. \n",
    "    triples = sorted(triples, key=lambda x: x[1])\n",
    "    \n",
    "    # Write out midi. \n",
    "    MyMIDI = MIDIFile(1)    # Create the MIDIFile Object with 1 track\n",
    "    track = 0   \n",
    "    time = 0\n",
    "    tempo = 120\n",
    "    beat_per_sec = 60. / float(tempo)\n",
    "    MyMIDI.addTrackName(track, time, \"Sample Track\")  # Add track name \n",
    "    MyMIDI.addTempo(track, time, tempo)   # Add track tempo\n",
    "    \n",
    "    for triple in triples:\n",
    "        (midi_pitch, bgn_fr, fin_fr) = triple\n",
    "        bgn_beat = bgn_fr * step_sec / float(beat_per_sec)\n",
    "        fin_beat = fin_fr * step_sec / float(beat_per_sec)\n",
    "        dur_beat = fin_beat - bgn_beat\n",
    "        MyMIDI.addNote(track=0,     # The track to which the note is added.\n",
    "                    channel=0,   # the MIDI channel to assign to the note. [Integer, 0-15]\n",
    "                    pitch=midi_pitch,    # the MIDI pitch number [Integer, 0-127].\n",
    "                    time=bgn_beat,      # the time (in beats) at which the note sounds [Float].\n",
    "                    duration=dur_beat,  # the duration of the note (in beats) [Float].\n",
    "                    volume=100)  # the volume (velocity) of the note. [Integer, 0-127].\n",
    "    out_file = open(out_path, 'wb')\n",
    "    MyMIDI.writeFile(out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fn_fp_tn(p_y_pred, y_gt, thres, average):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      p_y_pred: shape = (n_samples,) or (n_samples, n_classes)\n",
    "      y_gt: shape = (n_samples,) or (n_samples, n_classes)\n",
    "      thres: float between 0 and 1. \n",
    "      average: None (element wise) | 'micro' (calculate metrics globally) \n",
    "        | 'macro' (calculate metrics for each label then average). \n",
    "      \n",
    "    Returns:\n",
    "      tp, fn, fp, tn or list of tp, fn, fp, tn. \n",
    "    \"\"\"\n",
    "    if p_y_pred.ndim == 1:\n",
    "        y_pred = np.zeros_like(p_y_pred)\n",
    "        y_pred[np.where(p_y_pred > thres)] = 1.\n",
    "        tp = np.sum(y_pred + y_gt > 1.5)\n",
    "        fn = np.sum(y_gt - y_pred > 0.5)\n",
    "        fp = np.sum(y_pred - y_gt > 0.5)\n",
    "        tn = np.sum(y_pred + y_gt < 0.5)\n",
    "        return tp, fn, fp, tn\n",
    "    elif p_y_pred.ndim == 2:\n",
    "        tps, fns, fps, tns = [], [], [], []\n",
    "        n_classes = p_y_pred.shape[1]\n",
    "        for j1 in xrange(n_classes):\n",
    "            (tp, fn, fp, tn) = tp_fn_fp_tn(p_y_pred[:, j1], y_gt[:, j1], thres, None)\n",
    "            tps.append(tp)\n",
    "            fns.append(fn)\n",
    "            fps.append(fp)\n",
    "            tns.append(tn)\n",
    "        if average is None:\n",
    "            return tps, fns, fps, tns\n",
    "        elif average == 'micro' or average == 'macro':\n",
    "            return np.sum(tps), np.sum(fns), np.sum(fps), np.sum(tns)\n",
    "        else: \n",
    "            raise Exception(\"Incorrect average arg!\")\n",
    "    else:\n",
    "        raise Exception(\"Incorrect dimension!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_recall_fvalue(p_y_pred, y_gt, thres, average):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      p_y_pred: shape = (n_samples,) or (n_samples, n_classes)\n",
    "      y_gt: shape = (n_samples,) or (n_samples, n_classes)\n",
    "      thres: float between 0 and 1. \n",
    "      average: None (element wise) | 'micro' (calculate metrics globally) \n",
    "        | 'macro' (calculate metrics for each label then average). \n",
    "      \n",
    "    Returns:\n",
    "      prec, recall, fvalue | list or prec, recall, fvalue. \n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    if p_y_pred.ndim == 1:\n",
    "        (tp, fn, fp, tn) = tp_fn_fp_tn(p_y_pred, y_gt, thres, average=None)\n",
    "        prec = tp / max(float(tp + fp), eps)\n",
    "        recall = tp / max(float(tp + fn), eps)\n",
    "        fvalue = 2 * (prec * recall) / max(float(prec + recall), eps)\n",
    "        return prec, recall, fvalue\n",
    "    elif p_y_pred.ndim == 2:\n",
    "        n_classes = p_y_pred.shape[1]\n",
    "        if average is None or average == 'macro':\n",
    "            precs, recalls, fvalues = [], [], []\n",
    "            for j1 in xrange(n_classes):\n",
    "                (prec, recall, fvalue) = prec_recall_fvalue(p_y_pred[:, j1], y_gt[:, j1], thres, average=None)\n",
    "                precs.append(prec)\n",
    "                recalls.append(recall)\n",
    "                fvalues.append(fvalue)\n",
    "            if average is None:\n",
    "                return precs, recalls, fvalues\n",
    "            elif average == 'macro':\n",
    "                return np.mean(precs), np.mean(recalls), np.mean(fvalues)\n",
    "        elif average == 'micro':\n",
    "            (prec, recall, fvalue) = prec_recall_fvalue(p_y_pred.flatten(), y_gt.flatten(), thres, average=None)\n",
    "            return prec, recall, fvalue\n",
    "        else:\n",
    "            raise Exception(\"Incorrect average arg!\")\n",
    "    else:\n",
    "        raise Exception(\"Incorrect dimension!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DataGenerator(object):\n",
    "    def __init__(self, batch_size, type, te_max_iter=None):\n",
    "        assert type in ['train', 'test']\n",
    "        self._batch_size_ = batch_size\n",
    "        self._type_ = type\n",
    "        self._te_max_iter_ = te_max_iter\n",
    "        \n",
    "    def generate(self, xs, ys):\n",
    "        x = xs[0]\n",
    "        y = ys[0]\n",
    "        batch_size = self._batch_size_\n",
    "        n_samples = len(x)\n",
    "        \n",
    "        index = np.arange(n_samples)\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        iter = 0\n",
    "        epoch = 0\n",
    "        pointer = 0\n",
    "        while True:\n",
    "            if (self._type_ == 'test') and (self._te_max_iter_ is not None):\n",
    "                if iter == self._te_max_iter_:\n",
    "                    break\n",
    "            iter += 1\n",
    "            if pointer >= n_samples:\n",
    "                epoch += 1\n",
    "                if (self._type_) == 'test' and (epoch == 1):\n",
    "                    break\n",
    "                pointer = 0\n",
    "                np.random.shuffle(index)                \n",
    " \n",
    "            batch_idx = index[pointer : min(pointer + batch_size, n_samples)]\n",
    "            pointer += batch_size\n",
    "            yield x[batch_idx], y[batch_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_weights(m):\n",
    "    classname = m.__class__.__name__    \n",
    "    if classname.find('Linear') != -1:\n",
    "        scale = 0.1\n",
    "        m.weight.data = torch.nn.init.uniform(m.weight.data, -scale, scale)\n",
    "        m.bias.data.fill_(0.)\n",
    "\n",
    "def glorot_uniform_weights(m):\n",
    "    classname = m.__class__.__name__    \n",
    "    if classname.find('Linear') != -1:\n",
    "        # w = torch.nn.init.xavier_uniform(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        w = torch.nn.init.xavier_uniform(m.weight.data)\n",
    "        m.weight.data = w\n",
    "        m.bias.data.fill_(0.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, gen, xs, ys, cuda):\n",
    "    model.eval()\n",
    "    pred_all = []\n",
    "    y_all = []\n",
    "    for (batch_x, batch_y) in gen.generate(xs=xs, ys=ys):\n",
    "        batch_x = torch.Tensor(batch_x)\n",
    "        batch_x = Variable(batch_x, volatile=True)\n",
    "        if cuda:\n",
    "            batch_x = batch_x.cuda()\n",
    "        pred = model(batch_x)\n",
    "        pred = pred.data.cpu().numpy()\n",
    "        pred_all.append(pred)\n",
    "        y_all.append(batch_y)\n",
    "        \n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    (prec, recall, fvalue) = prec_recall_fvalue(pred_all, y_all, thres=0.5, average='micro')\n",
    "    \n",
    "        \n",
    "    print(\"prec: %f, recall: %f, fvalue: %f\" % (prec, recall, fvalue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_concat, n_freq, n_out):\n",
    "        super(Net, self).__init__()\n",
    "        n_in = n_concat * n_freq\n",
    "        n_hid = 500\n",
    "        \n",
    "        self.fc1 = nn.Linear(n_in, n_hid)\n",
    "        self.fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.fc3 = nn.Linear(n_hid, n_hid)\n",
    "        self.fc4 = nn.Linear(n_hid, n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        drop_p = 0.2\n",
    "        x1 = x.view(len(x), -1)\n",
    "        x2 = F.dropout(F.relu(self.fc1(x1)), p=drop_p, training=self.training)\n",
    "        x3 = F.dropout(F.relu(self.fc2(x2)), p=drop_p, training=self.training)\n",
    "        x4 = F.dropout(F.relu(self.fc3(x3)), p=drop_p, training=self.training)\n",
    "        x5 = F.sigmoid(self.fc4(x4))\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    cuda = args.use_cuda and torch.cuda.is_available()\n",
    "    workspace = args.workspace\n",
    "    feat_type = args.feat_type\n",
    "    lr = args.lr\n",
    "    resume_model_path = args.resume_model_path\n",
    "    script_na = args.script_na\n",
    "    print(\"cuda:\", cuda)\n",
    "\n",
    "    # Load data. \n",
    "    t1 = time.time()\n",
    "    tr_packed_feat_path = os.path.join(workspace, \"packed_features\", feat_type, \"train.p\")\n",
    "    te_packed_feat_path = os.path.join(workspace, \"packed_features\", feat_type, \"test.p\")\n",
    "    [tr_x_list, tr_y_list, _] = cPickle.load(open(tr_packed_feat_path, 'rb'))\n",
    "    [te_x_list, te_y_list, _] = cPickle.load(open(te_packed_feat_path, 'rb'))\n",
    "    print(\"Loading packed feature time: %s s\" % (time.time() - t1,))\n",
    "        \n",
    "    # Scale. \n",
    "    if True:\n",
    "        scale_path = os.path.join(workspace, \"scalers\", feat_type, \"scaler.p\")\n",
    "        scaler = pickle.load(open(scale_path, 'rb'))\n",
    "        tr_x_list = scale_on_x_list(tr_x_list, scaler)\n",
    "        te_x_list = scale_on_x_list(te_x_list, scaler)\n",
    "    \n",
    "    # Data to 3d. \n",
    "    n_concat = 3\n",
    "    n_hop = 1\n",
    "    (tr_x, tr_y) = data_to_3d(tr_x_list, tr_y_list, n_concat, n_hop)\n",
    "    (te_x, te_y) = data_to_3d(te_x_list, te_y_list, n_concat, n_hop)\n",
    "    n_freq = tr_x.shape[-1]\n",
    "    n_out = tr_y.shape[-1]\n",
    "    print(tr_x.shape, tr_y.shape)\n",
    "    \n",
    "    # Model. \n",
    "    model = Net(n_concat, n_freq, n_out)\n",
    "    \n",
    "    if os.path.isfile(resume_model_path):\n",
    "        # Load weights. \n",
    "        print(\"Loading checkpoint '%s'\" % resume_model_path)\n",
    "        checkpoint = torch.load(resume_model_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        iter = checkpoint['iter']\n",
    "    else:\n",
    "        # Randomly init weights. \n",
    "        print(\"Train from random initialization. \")\n",
    "        model.apply(glorot_uniform_weights)\n",
    "        iter = 0\n",
    "    \n",
    "    # Move model to GPU. \n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    # Optimizer. \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    \n",
    "    # Data Generator\n",
    "    batch_size = 500\n",
    "    tr_gen = DataGenerator(batch_size=batch_size, type='train')\n",
    "    eval_tr_gen = DataGenerator(batch_size=batch_size, type='test', te_max_iter=20)\n",
    "    eval_te_gen = DataGenerator(batch_size=batch_size, type='test')\n",
    "    \n",
    "    iters_per_epoch = len(tr_x) / batch_size\n",
    "    print(\"Iters_per_epoch: %d\" % iters_per_epoch)\n",
    "    \n",
    "    # Train. \n",
    "    eps = 1e-8\n",
    "    for (batch_x, batch_y) in tr_gen.generate(xs=[tr_x], ys=[tr_y]):\n",
    "        if iter % (1000) == 0:\n",
    "            print(\"\\n--- Evaluation of training set (subset), iteration: %d ---\" % iter)\n",
    "            eval(model, eval_tr_gen, [tr_x], [tr_y], cuda)\n",
    "            print(\"--- Evaluation of testing set, iteration: %d ---\" % iter)\n",
    "            eval(model, eval_te_gen, [te_x], [te_y], cuda)\n",
    "            print(\"-----------------------------------------------\\n\")\n",
    "        \n",
    "        # Move data to GPU. \n",
    "        t1 = time.time()\n",
    "        batch_x = torch.Tensor(batch_x)\n",
    "        batch_y = torch.Tensor(batch_y)\n",
    "        batch_x = Variable(batch_x)\n",
    "        batch_y = Variable(batch_y)\n",
    "        if cuda:\n",
    "            batch_x = batch_x.cuda()\n",
    "            batch_y = batch_y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        output = model(batch_x)\n",
    "        output = torch.clamp(output, eps, 1. - eps)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if iter % 200 == 0:\n",
    "            print(\"Iter: %d loss: %f\" % (iter, loss))\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        # Save model. \n",
    "        if iter % 1000 == 0:\n",
    "            save_out_dict = {'iter': iter, \n",
    "                             'state_dict': model.state_dict(), \n",
    "                             'optimizer': optimizer.state_dict(), }\n",
    "            save_out_path = os.path.join(workspace, \"models\", script_na, feat_type, \"md_%diters.tar\" % iter)\n",
    "            create_folder(os.path.dirname(save_out_path))\n",
    "            torch.save(save_out_dict, save_out_path)\n",
    "            print(\"Save model to %s\" % save_out_path)\n",
    "            \n",
    "        # Stop training. \n",
    "        if iter == 10001:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args):\n",
    "    cuda = args.use_cuda and torch.cuda.is_available()\n",
    "    workspace = args.workspace\n",
    "    model_name = args.model_name\n",
    "    feat_type = args.feat_type\n",
    "    script_na = args.script_na\n",
    "\n",
    "    # Load data. \n",
    "    te_packed_feat_path = os.path.join(workspace, \"packed_features\", feat_type, \"test.p\")\n",
    "    [te_x_list, te_y_list, te_na_list] = cPickle.load(open(te_packed_feat_path, 'rb'))\n",
    "        \n",
    "    # Scale. \n",
    "    if True:\n",
    "        scale_path = os.path.join(workspace, \"scalers\", feat_type, \"scaler.p\")\n",
    "        scaler = pickle.load(open(scale_path, 'rb'))\n",
    "        te_x_list = scale_on_x_list(te_x_list, scaler)\n",
    "        \n",
    "    # Construct model topology. \n",
    "    n_concat = 3\n",
    "    te_n_hop = 1\n",
    "    n_freq = te_x_list[0].shape[-1]\n",
    "    n_out = te_y_list[0].shape[-1]\n",
    "    model = Net(n_concat, n_freq, n_out)\n",
    "    \n",
    "    # Init the weights of model using trained weights. \n",
    "    model_path = os.path.join(workspace, \"models\", script_na, feat_type, model_name)\n",
    "    if os.path.isfile(model_path):\n",
    "        print(\"Loading checkpoint '%s'\" % model_path)\n",
    "        checkpoint = torch.load(model_path)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        raise Exception(\"Model path %s does not exist!\" % model_path)\n",
    "        \n",
    "    # Move model to GPU. \n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "        \n",
    "    # Directory to write out transcript midi files. \n",
    "    out_midi_dir = os.path.join(workspace, \"out_midis\", get_filename(__file__), feat_type)\n",
    "    create_folder(out_midi_dir)\n",
    "        \n",
    "    # Data to 3d. \n",
    "    n_half = (n_concat - 1) / 2\n",
    "    for i1 in xrange(len(te_x_list)):\n",
    "        x = te_x_list[i1]   # (n_time, n_freq)\n",
    "        y = te_y_list[i1]   # (n_time, n_out)\n",
    "        bare_na = os.path.splitext(te_na_list[i1])[0]\n",
    "        (n_time, n_freq) = x.shape\n",
    "        \n",
    "        zero_pad = np.zeros((n_half, n_freq))\n",
    "        x = np.concatenate((zero_pad, x, zero_pad), axis=0)\n",
    "        x3d = mat_2d_to_3d(x, n_concat, te_n_hop)     # (n_time, n_concat, n_freq)\n",
    "        \n",
    "        # Move data to GPU. \n",
    "        x3d = torch.Tensor(x3d)\n",
    "        x3d = Variable(x3d)\n",
    "        if cuda:\n",
    "            x3d = x3d.cuda()\n",
    "        \n",
    "        # Inference. \n",
    "        model.eval()\n",
    "        pred = model(x3d)   # (n_time, n_out)\n",
    "        \n",
    "        # Convert data type to numpy. \n",
    "        pred = pred.data.cpu().numpy()\n",
    "        \n",
    "        # Threshold and write out predicted piano roll to midi file. \n",
    "        mid_roll = prob_to_midi_roll(pred, 0.5)\n",
    "        out_path = os.path.join(out_midi_dir, \"%s.mid\" % bare_na)\n",
    "        print(\"Write out to: %s\" % out_path)\n",
    "        write_midi_roll_to_midi(mid_roll, out_path)\n",
    "        \n",
    "        # Debug plot. \n",
    "        if True:\n",
    "            fig, axs = plt.subplots(3,1, sharex=True)\n",
    "            axs[0].matshow(y.T, origin='lower', aspect='auto')\n",
    "            axs[1].matshow(pred.T, origin='lower', aspect='auto')\n",
    "            binary_pred = (np.sign(pred - 0.5) + 1) / 2\n",
    "            axs[2].matshow(binary_pred.T, origin='lower', aspect='auto')\n",
    "            axs[0].set_title(\"Ground truth\")\n",
    "            axs[1].set_title(\"DNN output probability\")\n",
    "            axs[2].set_title(\"DNN output probability after thresholding\")\n",
    "            for j1 in xrange(3):\n",
    "                axs[j1].set_ylabel('note index')\n",
    "                axs[j1].set_xlabel('frames')\n",
    "                axs[j1].xaxis.set_label_coords(1.06, -0.01)\n",
    "                axs[j1].xaxis.tick_bottom()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
